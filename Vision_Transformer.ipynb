{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess the CIFAR-10 dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]\n",
    "val_size = int(0.2 * len(x_train))\n",
    "x_val, y_val = x_train[:val_size], y_train[:val_size]\n",
    "x_train, y_train = x_train[val_size:], y_train[val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 4  # Larger patches for fewer tokens\n",
    "EMBED_DIM = 128  # Moderately sized embedding dimension\n",
    "NUM_HEADS = 4  # Balanced number of heads for multi-head attention\n",
    "NUM_LAYERS = 3  # Reduced number of layers to 3\n",
    "MLP_DIM = 256  # MLP dimension for transformer layers\n",
    "DROPOUT_RATE = 0.1\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 10  # Reduced epochs to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(images, patch_size):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=[1, patch_size, patch_size, 1],\n",
    "        strides=[1, patch_size, patch_size, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding='VALID'\n",
    "    )\n",
    "    patch_dim = patch_size * patch_size * 3\n",
    "    patches = tf.reshape(patches, [batch_size, -1, patch_dim])\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(num_patches, dim):\n",
    "    positions = np.arange(num_patches)[:, np.newaxis]\n",
    "    dimensions = np.arange(dim)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (dimensions // 2)) / np.float32(dim))\n",
    "    angle_rads = positions * angle_rates\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return tf.cast(angle_rads, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(x, num_heads, mlp_dim, dropout_rate):\n",
    "    # Multi-head self-attention\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=x.shape[-1])(x, x)\n",
    "    x = layers.LayerNormalization()(x + attn_output)\n",
    "    # Feedforward network\n",
    "    mlp_output = layers.Dense(mlp_dim, activation='relu')(x)\n",
    "    mlp_output = layers.Dropout(dropout_rate)(mlp_output)\n",
    "    mlp_output = layers.Dense(x.shape[-1])(mlp_output)\n",
    "    return layers.LayerNormalization()(x + mlp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit(num_patches, patch_dim, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout_rate):\n",
    "    inputs = layers.Input(shape=(num_patches, patch_dim))\n",
    "    # Linear projection of patches\n",
    "    x = layers.Dense(embed_dim)(inputs)\n",
    "    # Add positional encoding\n",
    "    pos_encoding = positional_encoding(num_patches, embed_dim)\n",
    "    x += pos_encoding\n",
    "    # Transformer layers\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_block(x, num_heads, mlp_dim, dropout_rate)\n",
    "    # Classification head\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patches = (32 // PATCH_SIZE) ** 2\n",
    "patch_dim = PATCH_SIZE * PATCH_SIZE * 3\n",
    "model = create_vit(\n",
    "    num_patches=num_patches,\n",
    "    patch_dim=patch_dim,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    mlp_dim=MLP_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    extract_patches(x_train, PATCH_SIZE), y_train,\n",
    "    validation_data=(extract_patches(x_val, PATCH_SIZE), y_val),\n",
    "    batch_size=64,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_patches = extract_patches(x_test, PATCH_SIZE)\n",
    "test_loss, test_accuracy = model.evaluate(test_patches, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "    \n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(test_patches), axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Classification Report (Precision, Recall, F1-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred, target_names=[str(i) for i in range(NUM_CLASSES)])\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Optional ROC curve and AUC for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_binarized = label_binarize(y_test, classes=[i for i in range(NUM_CLASSES)])\n",
    "y_pred_probs = model.predict(test_patches)\n",
    "\n",
    "# Plotting ROC Curves\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(NUM_CLASSES):\n",
    "    plt.plot(fpr[i], tpr[i], label=f\"Class {i} (AUC = {roc_auc[i]:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
